{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/项目流程图.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "# input file\n",
    "ZIP_DATA = os.path.join(root, 'data', '百度题库.zip')  # 要解压的文件\n",
    "STOPWORDS = os.path.join(root,  'data', 'stopwords.txt')\n",
    "\n",
    "# output file path\n",
    "TRAIN_TSV = os.path.join(root,  'data', 'train.tsv')  # BERT的数据文件\n",
    "DEV_TSV = os.path.join(root, 'data', 'dev.tsv')\n",
    "TEST_TSV = os.path.join(root, 'data', 'test.tsv')\n",
    "\n",
    "TOKENIZER_BINARIZER = os.path.join(root, 'data', 'tokenizer_binarizer.pickle')\n",
    "X_NPY = os.path.join(root,  'data', 'x.npy')  # testcnn 和 transformer的数据文件\n",
    "Y_NPY = os.path.join(root,  'data', 'y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压数据\n",
    "def unzip_data():\n",
    "    \"\"\"\n",
    "    :param file: 要解压的文件夹的路径\n",
    "    :param unzip_dir: 解压到哪里\n",
    "    :return: 解压后产生的文件夹的路径\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(ZIP_DATA, 'r') as z:\n",
    "        z.extractall(os.path.join(root,  'data'))\n",
    "        print(\"已将压缩包解压至{}\".format(z.filename.rstrip('.zip')))\n",
    "        return z.filename.rstrip('.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将压缩包解压至E:\\GitHub\\Text-Classification\\data\\百度题库\n"
     ]
    }
   ],
   "source": [
    "data_path = unzip_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合数据\n",
    "\n",
    "![](../images/原始数据概况.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集包含4个科目的题目，每门科目下又有不同的主题。如**历史-古代史(1000)** 括号内的数字表示有1000道题目\n",
    "\n",
    "同时，每道题目有许多知识点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>web-scraper-order</th>\n",
       "      <th>web-scraper-start-url</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1566523436-2497</td>\n",
       "      <td>https://study.baidu.com/tiku</td>\n",
       "      <td>[题目]\\n据《左传》记载，春秋后期鲁国大夫季孙氏的家臣阳虎独掌权柄后，标榜要替鲁国国君整肃...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1566523436-2506</td>\n",
       "      <td>https://study.baidu.com/tiku</td>\n",
       "      <td>[题目]\\n秦始皇统一六国后创制了一套御玺。如任命国家官员，则封印“皇帝之玺”；若任命四夷的...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  web-scraper-order         web-scraper-start-url  \\\n",
       "0   1566523436-2497  https://study.baidu.com/tiku   \n",
       "1   1566523436-2506  https://study.baidu.com/tiku   \n",
       "\n",
       "                                                item  \n",
       "0  [题目]\\n据《左传》记载，春秋后期鲁国大夫季孙氏的家臣阳虎独掌权柄后，标榜要替鲁国国君整肃...  \n",
       "1  [题目]\\n秦始皇统一六国后创制了一套御玺。如任命国家官员，则封印“皇帝之玺”；若任命四夷的...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exm_file = os.path.join(data_path, '高中_历史', 'origin', '古代史.csv')  # 举个栗子\n",
    "exm = pd.read_csv(exm_file)\n",
    "exm.head(2)  # 展示数据前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[题目]\\n据《左传》记载，春秋后期鲁国大夫季孙氏的家臣阳虎独掌权柄后，标榜要替鲁国国君整肃跋扈的大夫，此举不仅得不到知礼之士的赞成，反而受到批评。因为此举（  ）A. 挑战了宗法制度B. 损害了大夫利益C. 冲击了天子权威D. 不符合周礼规定题型: 单选题|难度: 一般|使用次数: 0|纠错复制收藏到空间加入选题篮查看答案解析答案：D解析：阳虎的身份是鲁国大夫、季孙氏的家臣，按周礼的规定，他效忠于季孙氏，而他标榜为鲁国国君整肃大夫即是僭越，所以受到批评，故违背了周礼，故选择D项。宗法制度以血缘为核心，故A项与此无关，排除；B项与题意无关，排除；材料的事件涉及鲁国国内，与周天子权威无关，排除C项。知识点：\\n[知识点：]\\n“重农抑商”政策,郡县制,夏商两代的政治制度,中央官制——三公九卿制,皇帝制度'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一题的内容\n",
    "exm.loc[0, 'item']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上面第一题的内容：**结尾部分**会有知识点，所以我们需要提取出知识点。用一个list装起来\n",
    "> “\\n[知识点：]\\n“重农抑商”政策,郡县制,夏商两代的政治制度,中央官制——三公九卿制,皇帝制度'”\n",
    "\n",
    "`[“重农抑商”政策, 郡县制, 夏商两代的政治制度, 中央官制——三公九卿制, 皇帝制度]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(data_path):\n",
    "    \"\"\"\n",
    "    把四门科目内的所有文件合并\n",
    "    \"\"\"\n",
    "    r = re.compile(r'\\[知识点：\\]\\n(.*)')  # 用来寻找知识点的正则表达式\n",
    "    r1 = re.compile(r'纠错复制收藏到空间加入选题篮查看答案解析|\\n|知识点：|\\s|\\[题目\\]')  # 简单清洗\n",
    "    \n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        if files:  # 如果文件夹下有csv文件\n",
    "            for f in files:\n",
    "                subject = re.findall('高中_(.{2})', root)[0]\n",
    "                topic = f.strip('.csv')\n",
    "                tmp = pd.read_csv(os.path.join(root, f))  # 打开csv文件\n",
    "                tmp['subject'] = subject  # 主标签：科目\n",
    "                tmp['topic'] = topic  # 副标签：科目下主题\n",
    "                tmp['knowledge'] = tmp['item'].apply(lambda x: r.findall(x)[0].replace(',', ' ') if r.findall(x) else '')\n",
    "                tmp['item'] = tmp['item'].apply(lambda x: r1.sub('', r.sub('', x)))\n",
    "                data.append(tmp)\n",
    "                \n",
    "    data = pd.concat(data).rename(columns={'item': 'content'}).reset_index(drop=True)\n",
    "    # 删掉多余的两列\n",
    "    data.drop(['web-scraper-order', 'web-scraper-start-url'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "      <th>topic</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28772</th>\n",
       "      <td>细胞内糖分解代谢过程如图，下列叙述错误的是（）A植物细胞能进行过程①和③或过程①和④B真核细...</td>\n",
       "      <td>生物</td>\n",
       "      <td>稳态与环境</td>\n",
       "      <td>细胞免疫的概念和过程 无氧呼吸的概念与过程 无氧呼吸的类型 有氧呼吸的三个阶段</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28635</th>\n",
       "      <td>正常情况下，人体进食后血液内胰岛素含量和胰高血糖素的含量变化情况分别是（）A.减少，增加B....</td>\n",
       "      <td>生物</td>\n",
       "      <td>稳态与环境</td>\n",
       "      <td>人体的体温调节 人体水盐平衡调节 血糖平衡的调节</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8993</th>\n",
       "      <td>北京和广州两地的自转角速度和线速度相比较，正确的叙述是（）A两地的角速度和线速度都相同B两地...</td>\n",
       "      <td>地理</td>\n",
       "      <td>宇宙中的地球</td>\n",
       "      <td>地球运动的基本形式</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content subject   topic  \\\n",
       "28772  细胞内糖分解代谢过程如图，下列叙述错误的是（）A植物细胞能进行过程①和③或过程①和④B真核细...      生物   稳态与环境   \n",
       "28635  正常情况下，人体进食后血液内胰岛素含量和胰高血糖素的含量变化情况分别是（）A.减少，增加B....      生物   稳态与环境   \n",
       "8993   北京和广州两地的自转角速度和线速度相比较，正确的叙述是（）A两地的角速度和线速度都相同B两地...      地理  宇宙中的地球   \n",
       "\n",
       "                                     knowledge  \n",
       "28772  细胞免疫的概念和过程 无氧呼吸的概念与过程 无氧呼吸的类型 有氧呼吸的三个阶段  \n",
       "28635                 人体的体温调节 人体水盐平衡调节 血糖平衡的调节  \n",
       "8993                                 地球运动的基本形式  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combine_data(data_path)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取标签\n",
    "先来看一下知识点出现的频率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有919个标签\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "knowledges = ' '.join(df['knowledge']).split()  # 合并\n",
    "knowledges = Counter(knowledges)\n",
    "print('一共有%d个标签' % len(knowledges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('人工授精、试管婴儿等生殖技术', 4402),\n",
       " ('生物性污染', 4402),\n",
       " ('避孕的原理和方法', 4402),\n",
       " ('遗传的细胞基础', 2487),\n",
       " ('遗传的分子基础', 2455)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledges.most_common()[:5]  # 频率最高的5个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('酶的发现历程', 1),\n",
       " ('植物色素的提取', 1),\n",
       " ('探究水族箱（或鱼缸）中群落的演替', 1),\n",
       " ('证明DNA是主要遗传物质的实验', 1),\n",
       " ('生物多样性形成的影响因素', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledges.most_common()[-5:]  # 频率最低的5个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这个差别还是很大的，如果把所有低频标签也考虑了，分类效果肯定不好，因为在划分训练测试集的时候很可能样本只出现在训练集或只出现在测试集，于是下面过滤掉一些低频标签。我这里取的是1%。即**过滤掉出现次数低于样本数的1%的标签**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(df, freq=0.01):\n",
    "    \"\"\"\n",
    "\n",
    "    :param df: 合并后的数据集\n",
    "    :param freq: 要过滤的标签占样本数量的比例\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    knowledges = ' '.join(df['knowledge']).split()  # 合并\n",
    "    knowledges = Counter(knowledges)\n",
    "    k = int(df.shape[0] * freq)  # 计算对应频率知识点出现的次数\n",
    "    print('过滤掉出现次数少于 %d 次的标签' % k)\n",
    "    top_k = {i for i in knowledges if knowledges[i] > k}  # 过滤掉知识点出现次数小于k的样本\n",
    "    df.knowledge = df.knowledge.apply(lambda x: ' '.join([label for label in x.split() if label in top_k]))\n",
    "    df['label'] = df[['subject', 'topic', 'knowledge']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    \n",
    "    return df[['label', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤掉出现次数少于 298 次的标签\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17241</th>\n",
       "      <td>生物 分子与细胞 生命活动离不开细胞</td>\n",
       "      <td>下列关于人体细胞的叙述，错误的是（）A.人的正常体细胞的分裂次数是有限的B.自由基攻击蛋白质...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25766</th>\n",
       "      <td>生物 稳态与环境</td>\n",
       "      <td>下列叙述中，不属于种群空间特征描述的是（）A.斑马在草原上成群活动B.每毫升河水中有9个大肠...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    label                                            content\n",
       "17241  生物 分子与细胞 生命活动离不开细胞  下列关于人体细胞的叙述，错误的是（）A.人的正常体细胞的分裂次数是有限的B.自由基攻击蛋白质...\n",
       "25766           生物 稳态与环境   下列叙述中，不属于种群空间特征描述的是（）A.斑马在草原上成群活动B.每毫升河水中有9个大肠..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_label(df)\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对于bert 的预处理\n",
    "从这里就开始产生分支了，由于bert自带了文本预处理的工具，所以这里只需要按照bert读取文件的方式生成训练、验证、测试集即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_data(df, small=False):\n",
    "    \"\"\"\n",
    "    如果small=True：是因为自己的电脑太菜，就用比较小的数据量在本地实现模型\n",
    "    该函数给bert模型划分了3个数据集\n",
    "    \"\"\"\n",
    "    df['content'] = df['content'].apply(lambda x:x.replace(' ', ''))\n",
    "    if small:\n",
    "        print('use small dataset to test my local bert model really work')\n",
    "        train = df.sample(128)\n",
    "        dev = df.sample(64)\n",
    "        test = df.sample(64)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=0.2, random_state=2020)\n",
    "        train, dev = train_test_split(train, test_size=0.2, random_state=2020)\n",
    "\n",
    "    print('preprocess for bert!')\n",
    "    print('create 3 tsv file(train, dev, test) in %s' % (os.path.join(root,  'data')))\n",
    "    train.to_csv(TRAIN_TSV, index=None, sep='\\t')\n",
    "    dev.to_csv(DEV_TSV, index=None, sep='\\t')\n",
    "    test.to_csv(TEST_TSV, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess for bert!\n",
      "create 3 tsv file(train, dev, test) in E:\\GitHub\\Text-Classification\\data\n"
     ]
    }
   ],
   "source": [
    "create_bert_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestCNN和Transformer的预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下的数据预处理流程：\n",
    "![](../images/数据预处理part2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **去标点**，**切词**，**去停用词**\n",
    "合并到`sentence_preprocess`这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "    return {line.strip() for line in open(STOPWORDS, encoding='UTF-8').readlines()}\n",
    "\n",
    "def sentence_preprocess(sentence):\n",
    "    # 去标点\n",
    "    r = re.compile(\"[^\\u4e00-\\u9fa5]+|题目\")\n",
    "    sentence = r.sub(\"\", sentence)  # 删除所有非汉字字符\n",
    "    \n",
    "    # 切词\n",
    "    words = jieba.cut(sentence, cut_all=False)  \n",
    "    \n",
    "    # 去停用词\n",
    "    stop_words = load_stopwords()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return words\n",
    "\n",
    "def df_preprocess(df):\n",
    "    df.content = df.content.apply(sentence_preprocess)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Light\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.986 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['左传', '记载', '春秋', '后期', '鲁国', '大夫', '季孙氏', '家臣', '阳虎', '独掌', '权柄', '后', '标榜', '鲁国', '国君', '整肃', '跋扈', '大夫']\n"
     ]
    }
   ],
   "source": [
    "# 展示一下结果\n",
    "sentence = '据《左传》记载，春秋后期鲁国大夫季孙氏的家臣阳虎独掌权柄后，标榜要替鲁国国君整肃跋扈的大夫'\n",
    "words = sentence_preprocess(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df_preprocess(df)  # 40s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字转token, padding,划分数据集\n",
    "这里使用keras和sklearn的工具包\n",
    "- `Tokenizer` 用于把文本转换成数字\n",
    "- `pad_sequences` 用于对齐文本\n",
    "- `MultiLabelBinarizer` 用于把标签转化为0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5, oov_token=\"<UNK>\")\n",
    "words = [['我们', '今天', '一起', '学习'],\n",
    "         ['我们', '今天', '玩'],\n",
    "         ['他们', '学习']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1, 4], [2, 3, 1], [1, 4]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(words)\n",
    "seqs = tokenizer.texts_to_sequences(words)\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1, '我们': 2, '今天': 3, '学习': 4, '一起': 5, '玩': 6, '他们': 7}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<UNK>', 2: '我们', 3: '今天', 4: '学习', 5: '一起', 6: '玩', 7: '他们'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pad_sequences演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 1],\n",
       "       [2, 3, 1],\n",
       "       [1, 4, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 长的截断，短的补0\n",
    "pad_sequences(seqs, maxlen=3, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiLabelBinarizer演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 1, 1, 1],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels = [['A', 'B'],\n",
    "          ['A', 'D'],\n",
    "          ['B', 'C', 'D'],\n",
    "          ['C']]\n",
    "mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编写代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演示完毕后开始编写代码，目的是使`df`里的文本全部变成数值型数据，为了后续给模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>历史 古代史 “重农抑商”政策 郡县制 夏商两代的政治制度 中央官制——三公九卿制 皇帝制度</td>\n",
       "      <td>[左传, 记载, 春秋, 后期, 鲁国, 大夫, 季孙氏, 家臣, 阳虎, 独掌, 权柄, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>历史 古代史 “重农抑商”政策 郡县制 夏商两代的政治制度 中央官制——三公九卿制 皇帝制度</td>\n",
       "      <td>[秦始皇, 统一, 六国后, 创制, 一套, 御玺, 任命, 国家, 官员, 封印, 皇帝,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            label  \\\n",
       "0  历史 古代史 “重农抑商”政策 郡县制 夏商两代的政治制度 中央官制——三公九卿制 皇帝制度   \n",
       "1  历史 古代史 “重农抑商”政策 郡县制 夏商两代的政治制度 中央官制——三公九卿制 皇帝制度   \n",
       "\n",
       "                                             content  \n",
       "0  [左传, 记载, 春秋, 后期, 鲁国, 大夫, 季孙氏, 家臣, 阳虎, 独掌, 权柄, ...  \n",
       "1  [秦始皇, 统一, 六国后, 创制, 一套, 御玺, 任命, 国家, 官员, 封印, 皇帝,...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testcnn_data(df, num_words=50000, maxlen=128):\n",
    "    \n",
    "    # 对于label处理\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(df.label.apply(lambda x: x.split()))\n",
    "    \n",
    "    # 对content处理\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(df.content.tolist())\n",
    "    x = tokenizer.texts_to_sequences(df.content)\n",
    "    x = pad_sequences(x, maxlen=maxlen, padding='post', truncating='post')   # padding\n",
    "    \n",
    "    # 保存数据\n",
    "    np.save(X_NPY, x)\n",
    "    np.save(Y_NPY, y)\n",
    "    print('已创建并保存x,y至：\\n {} \\n {}'.format(X_NPY, Y_NPY))\n",
    "    \n",
    "    # 同时还要保存tokenizer和 multi_label_binarizer\n",
    "    # 否则训练结束后无法还原把数字还原成文本\n",
    "    tb = {'tokenizer': tokenizer, 'binarizer': mlb}  # 用个字典来保存\n",
    "    with open(TOKENIZER_BINARIZER, 'wb') as f:\n",
    "        pickle.dump(tb, f)\n",
    "    print('已创建并保存tokenizer和binarizer至：\\n {}'.format(TOKENIZER_BINARIZER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testcnn_data():\n",
    "    \"\"\"\n",
    "    如果分开保存，那要保存6个文件太麻烦了。\n",
    "    所以采取读取之后划分数据集的方式\n",
    "    \"\"\"\n",
    "    # 与之前的bert同步\n",
    "    x = np.load(X_NPY)\n",
    "    y = np.load(Y_NPY)\n",
    "    \n",
    "    # 与之前bert的划分方式统一\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=2020)\n",
    "    train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.2, random_state=2020)\n",
    "    \n",
    "    return train_x, dev_x, test_x, train_y, dev_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取tokenizer 和 binarizer\n",
    "def load_tokenizer_binarizer():\n",
    "    with open(TOKENIZER_BINARIZER, 'rb') as f:\n",
    "        tb = pickle.load(f)\n",
    "    return tb['tokenizer'], tb['binarizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已创建并保存x,y至：\n",
      " E:\\GitHub\\Text-Classification\\data\\x.npy \n",
      " E:\\GitHub\\Text-Classification\\data\\y.npy\n",
      "已创建并保存tokenizer和binarizer至：\n",
      " E:\\GitHub\\Text-Classification\\data\\tokenizer_binarizer.pickle\n"
     ]
    }
   ],
   "source": [
    "create_testcnn_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19080, 128), (4770, 128), (5963, 128), (19080, 97), (4770, 97), (5963, 97))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, dev_x, test_x, train_y, dev_y, test_y = load_testcnn_data()\n",
    "train_x.shape, dev_x.shape, test_x.shape, train_y.shape, dev_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3717,   627,  1846, ...,    59,    51,    52],\n",
       "       [   47,   327,    62, ...,     0,     0,     0],\n",
       "       [17430, 35229,   932, ...,   173,   218,   116],\n",
       "       ...,\n",
       "       [ 7211,    64,    35, ...,   300,     3,   113],\n",
       "       [ 3448,  6444,   424, ...,   306,     0,     0],\n",
       "       [ 3600,    58,   287, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, mlb = load_tokenizer_binarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回顾整个数据预处理流程\n",
    "\n",
    "![](../images/项目流程图.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
